[
  {
    "title": "Pandas: Cargar CSV Gigante (Chunks)",
    "description": "Técnica eficiente para procesar archivos CSV de varios GBs usando chunks para no saturar la RAM.",
    "language": "python",
    "code": "import pandas as pd\n\nchunk_size = 10000\nchunks = []\n\n# Iterar sobre el archivo en bloques\nfor chunk in pd.read_csv('big_data.csv', chunksize=chunk_size):\n    # Procesar cada bloque (ej: filtrar)\n    filtered_chunk = chunk[chunk['price'] > 100]\n    chunks.append(filtered_chunk)\n\n# Unir resultados finales\ndf_final = pd.concat(chunks)\nprint(f'Filas totales cargadas: {len(df_final)}')",
    "category": "pandas",
    "tags": ["big-data", "performance", "etl"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Scraping: Rotación de User-Agents",
    "description": "Evita bloqueos en tus scrapers rotando el User-Agent en cada petición.",
    "language": "python",
    "code": "import requests\nimport random\n\nuser_agents = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15...',\n    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36...'\n]\n\nurl = 'https://site.com'\nheaders = {'User-Agent': random.choice(user_agents)}\n\nresponse = requests.get(url, headers=headers)\nprint(f'Status: {response.status_code} con UA: {headers[\"User-Agent\"][:20]}...')",
    "category": "scraping",
    "tags": ["security", "automation", "requests"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "SQL: Cohort Analysis (Retención)",
    "description": "Consulta avanzada para calcular la retención de usuarios por mes de registro (Cohortes).",
    "language": "sql",
    "code": "WITH cohorts AS (\n    SELECT user_id, DATE_TRUNC('month', MIN(created_at)) as cohort_month\n    FROM users\n    GROUP BY user_id\n),\nuser_activities AS (\n    SELECT user_id, DATE_TRUNC('month', activity_date) as activity_month\n    FROM logins\n)\nSELECT \n    c.cohort_month,\n    ua.activity_month,\n    COUNT(DISTINCT c.user_id) as active_users\nFROM cohorts c\nJOIN user_activities ua ON c.user_id = ua.user_id\nGROUP BY 1, 2\nORDER BY 1, 2;",
    "category": "db",
    "tags": ["analytics", "kpi", "retention"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Pandas: One-Hot Encoding",
    "description": "Transformar variables categóricas en numéricas (dummies) para Machine Learning.",
    "language": "python",
    "code": "import pandas as pd\n\ndf = pd.DataFrame({'color': ['red', 'blue', 'green', 'blue', 'red']})\n\n# Crear columnas binarias\ndf_encoded = pd.get_dummies(df, columns=['color'], prefix='is')\n\nprint(df_encoded.head())\n# Output: is_red, is_blue, is_green (0 o 1)",
    "category": "pandas",
    "tags": ["ml", "preprocessing", "datascience"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Selenium: Scroll Infinito",
    "description": "Script para hacer scroll automático hacia abajo en páginas dinámicas (ej: redes sociales).",
    "language": "python",
    "code": "from selenium import webdriver\nimport time\n\ndriver = webdriver.Chrome()\ndriver.get('https://infinite-scroll-site.com')\n\nlast_height = driver.execute_script('return document.body.scrollHeight')\n\nwhile True:\n    # Scroll hasta el fondo\n    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n    time.sleep(2) # Esperar carga\n    \n    new_height = driver.execute_script('return document.body.scrollHeight')\n    if new_height == last_height:\n        break # Fin de la página\n    last_height = new_height",
    "category": "scraping",
    "tags": ["selenium", "automation", "dynamic-web"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Pandas: Pivot Table Avanzada",
    "description": "Resumir ventas por Región y Producto calculando suma y promedio simultáneamente.",
    "language": "python",
    "code": "import pandas as pd\n\n# df = pd.read_csv('sales.csv')\n\npivot = pd.pivot_table(\n    df,\n    values='amount',\n    index=['region', 'manager'],\n    columns='product',\n    aggfunc=['sum', 'mean'],\n    fill_value=0\n)\n\nprint(pivot)",
    "category": "pandas",
    "tags": ["reporting", "aggregation", "excel-like"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "SQL: Moving Average (7 días)",
    "description": "Calcular el promedio móvil de ventas de los últimos 7 días usando Window Functions.",
    "language": "sql",
    "code": "SELECT \n    sale_date,\n    daily_sales,\n    AVG(daily_sales) OVER (\n        ORDER BY sale_date\n        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n    ) as moving_avg_7d\nFROM sales_summary\nORDER BY sale_date;",
    "category": "db",
    "tags": ["time-series", "window-functions", "trends"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "BeautifulSoup: Extraer Tablas HTML",
    "description": "Convertir cualquier tabla <table> de una web directamente a una lista de diccionarios.",
    "language": "python",
    "code": "from bs4 import BeautifulSoup\nimport requests\n\nhtml = requests.get('https://site.com/stats').text\nsoup = BeautifulSoup(html, 'html.parser')\ntable = soup.find('table', {'id': 'stats-table'})\n\ndata = []\nheaders = [th.text.strip() for th in table.find_all('th')]\n\nfor row in table.find_all('tr')[1:]:\n    cols = row.find_all('td')\n    row_data = {headers[i]: cols[i].text.strip() for i in range(len(cols))}\n    data.append(row_data)\n\nprint(data[:2])",
    "category": "scraping",
    "tags": ["bs4", "html-parsing", "structure"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "R: Dplyr Join & Filter",
    "description": "Unir dos dataframes y filtrar resultados en una sola tubería (pipe).",
    "language": "r",
    "code": "library(dplyr)\n\nresult <- users %>%\n  inner_join(orders, by = 'user_id') %>%\n  filter(order_status == 'completed') %>%\n  select(user_name, order_total, date) %>%\n  arrange(desc(order_total))\n\nhead(result)",
    "category": "pandas",
    "tags": ["r-stats", "tidyverse", "joins"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Python: Descargar Imágenes",
    "description": "Función robusta para descargar y guardar imágenes desde URLs.",
    "language": "python",
    "code": "import requests\nimport shutil\n\ndef download_image(url, filename):\n    r = requests.get(url, stream=True)\n    if r.status_code == 200:\n        with open(filename, 'wb') as f:\n            r.raw.decode_content = True\n            shutil.copyfileobj(r.raw, f)\n        print('Imagen guardada')\n    else:\n        print('Error al descargar')\n\ndownload_image('https://site.com/logo.png', 'logo_local.png')",
    "category": "scraping",
    "tags": ["files", "media", "requests"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Pandas: Manejo de Fechas",
    "description": "Extraer día, mes, año y día de la semana de una columna datetime.",
    "language": "python",
    "code": "import pandas as pd\n\ndf['date'] = pd.to_datetime(df['date_str'])\n\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month_name()\ndf['day_of_week'] = df['date'].dt.day_name()\ndf['is_weekend'] = df['date'].dt.dayofweek >= 5\n\nprint(df[['date', 'day_of_week', 'is_weekend']].head())",
    "category": "pandas",
    "tags": ["time-series", "feature-engineering", "dates"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "SQL: Ranking por Categoría",
    "description": "Encontrar el producto más vendido de cada categoría (Top 1).",
    "language": "sql",
    "code": "SELECT *\nFROM (\n    SELECT \n        product_name,\n        category,\n        sales,\n        RANK() OVER (PARTITION BY category ORDER BY sales DESC) as ranking\n    FROM products\n) t\nWHERE ranking = 1;",
    "category": "db",
    "tags": ["window-functions", "reporting", "advanced-sql"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Scrapy: Spider Básico",
    "description": "Estructura mínima de una araña Scrapy para rastrear títulos de un blog.",
    "language": "python",
    "code": "import scrapy\n\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://blog.scrapinghub.com']\n\n    def parse(self, response):\n        for title in response.css('.post-header>h2'):\n            yield {'title': title.css('a ::text').get()}\n\n        for next_page in response.css('a.next-posts-link'):\n            yield response.follow(next_page, self.parse)",
    "category": "scraping",
    "tags": ["scrapy", "framework", "crawling"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Python: Matplotlib Histograma",
    "description": "Visualización de distribución de datos con Matplotlib.",
    "language": "python",
    "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.normal(0, 1, 1000)\n\nplt.hist(data, bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribución Normal')\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.show()",
    "category": "pandas",
    "tags": ["visualization", "plotting", "data-science"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Pandas: Limpieza de Texto (Regex)",
    "description": "Limpiar columna de precios (quitar símbolos de moneda) y convertir a float.",
    "language": "python",
    "code": "import pandas as pd\n\ndf = pd.DataFrame({'price': ['$1,200.50', '€500', '$10.00']})\n\n# Eliminar todo lo que no sea dígito o punto\ndf['price_clean'] = df['price'].replace(r'[^\\d.]', '', regex=True)\n\n# Convertir a numérico\ndf['amount'] = pd.to_numeric(df['price_clean'])\n\nprint(df)",
    "category": "pandas",
    "tags": ["cleaning", "regex", "etl"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "SQL: JSON Extract (Postgres)",
    "description": "Consultar valores dentro de una columna tipo JSONB.",
    "language": "sql",
    "code": "SELECT \n    id,\n    data->>'name' as user_name,\n    data->'settings'->>'theme' as theme\nFROM \n    user_logs\nWHERE \n    data->>'status' = 'active';",
    "category": "db",
    "tags": ["json", "postgres", "nosql-in-sql"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Selenium: Manejo de Popups/Alerts",
    "description": "Cómo aceptar o rechazar alertas nativas del navegador.",
    "language": "python",
    "code": "from selenium import webdriver\n\ndriver = webdriver.Chrome()\n\n# Trigger de la alerta\ndriver.execute_script(\"alert('Hola');\")\n\n# Cambiar el foco a la alerta\nalert = driver.switch_to.alert\nprint(f\"Texto alerta: {alert.text}\")\n\n# Aceptar\nalert.accept()",
    "category": "scraping",
    "tags": ["selenium", "interaction", "testing"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "NumPy: Operaciones Vectorizadas",
    "description": "Cálculos matemáticos rápidos sobre arrays sin bucles for.",
    "language": "python",
    "code": "import numpy as np\n\nprecios = np.array([10, 20, 30])\ncantidades = np.array([2, 5, 1])\n\n# Multiplicación elemento a elemento (mucho más rápido que loops)\ntotales = precios * cantidades\n\nprint(f\"Total ventas: {totales.sum()}\")",
    "category": "pandas",
    "tags": ["numpy", "math", "performance"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Pandas: Merge (VLOOKUP style)",
    "description": "Unir dos tablas basándose en una columna clave (equivalente a buscarv).",
    "language": "python",
    "code": "import pandas as pd\n\nproducts = pd.DataFrame({'id': [1, 2], 'name': ['Laptop', 'Mouse']})\norders = pd.DataFrame({'order_id': [101, 102], 'prod_id': [2, 1]})\n\nmerged = pd.merge(\n    orders, \n    products, \n    left_on='prod_id', \n    right_on='id', \n    how='left'\n)\n\nprint(merged)",
    "category": "pandas",
    "tags": ["joins", "database", "excel"],
    "in_community": true,
    "is_public": true
  },
  {
    "title": "Python: Retry Decorator",
    "description": "Reintentar una función de scraping automáticamente si falla.",
    "language": "python",
    "code": "import time\nfrom functools import wraps\n\ndef retry(max_retries=3, delay=2):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for i in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    print(f\"Intento {i+1} falló: {e}\")\n                    time.sleep(delay)\n            raise Exception(\"Falló tras reintentos\")\n        return wrapper\n    return decorator",
    "category": "scraping",
    "tags": ["robustness", "decorators", "error-handling"],
    "in_community": true,
    "is_public": true
  }
]